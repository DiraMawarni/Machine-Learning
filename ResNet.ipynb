{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiraMawarni/Machine-Learning/blob/main/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Residual Networks (ResNet)**"
      ],
      "metadata": {
        "id": "IhYx1ggKzjKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we design increasingly deeper networks it becomes imperative to understand how adding layers can increase the complexity and expressiveness of the network."
      ],
      "metadata": {
        "id": "KOl8Rl_czokb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Residual Blocks"
      ],
      "metadata": {
        "id": "i73A_zjqzq9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l\n",
        "\n",
        "\n",
        "class Residual(nn.Module):  \n",
        "    \"\"\"The Residual block of ResNet.\"\"\"\n",
        "    def __init__(self, input_channels, num_channels,\n",
        "                 use_1x1conv=False, strides=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
        "                               kernel_size=3, padding=1, stride=strides)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
        "                               kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
        "                                   kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)"
      ],
      "metadata": {
        "id": "m4Gj7eZBzvmV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us look at a situation where the input and output are of the same shape."
      ],
      "metadata": {
        "id": "beuur8P0zyVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blk = Residual(3,3)\n",
        "X = torch.rand(4, 3, 6, 6)\n",
        "Y = blk(X)\n",
        "Y.shape"
      ],
      "metadata": {
        "id": "BxmYJbrjz07V",
        "outputId": "ed19b31a-e487-4461-9bf4-b23e898ea2b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 6, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have the option to halve the output height and width while increasing the number of output channels."
      ],
      "metadata": {
        "id": "5VtRbSkZz3vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blk = Residual(3,6, use_1x1conv=True, strides=2)\n",
        "blk(X).shape"
      ],
      "metadata": {
        "id": "tnBxx1aoz7lj",
        "outputId": "abe692b6-c900-4cf7-837e-b2f3ca2f4fd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 6, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ResNet Model**"
      ],
      "metadata": {
        "id": "BhnNYpr8z-HF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first two layers of ResNet are the same as those of the GoogLeNet we described before: the 7×7 convolutional layer with 64 output channels and a stride of 2 is followed by the 3×3 maximum pooling layer with a stride of 2. The difference is the batch normalization layer added after each convolutional layer in ResNet."
      ],
      "metadata": {
        "id": "8dAF9Qhj0C2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
        "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ],
      "metadata": {
        "id": "8QwnX2rL0Fo_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet uses four modules made up of residual blocks, each of which uses several residual blocks with the same number of output channels.\n",
        "\n",
        "Now, we implement this module. Note that special processing has been performed on the first module."
      ],
      "metadata": {
        "id": "SaXNBfTk0IJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet_block(input_channels, num_channels, num_residuals,\n",
        "                 first_block=False):\n",
        "    blk = []\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block:\n",
        "            blk.append(Residual(input_channels, num_channels,\n",
        "                                use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(Residual(num_channels, num_channels))\n",
        "    return blk"
      ],
      "metadata": {
        "id": "u83mzIO70Kp4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we add all the modules to ResNet. Here, two residual blocks are used for each module."
      ],
      "metadata": {
        "id": "Wugp8RQa0M86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
        "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
        "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
        "b5 = nn.Sequential(*resnet_block(256, 512, 2))"
      ],
      "metadata": {
        "id": "lamjL8q10PHg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, just like GoogLeNet, we add a global average pooling layer, followed by the fully-connected layer output.\n",
        "\n"
      ],
      "metadata": {
        "id": "xl8mBiRC0TP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(b1, b2, b3, b4, b5,\n",
        "                    nn.AdaptiveAvgPool2d((1,1)),\n",
        "                    nn.Flatten(), nn.Linear(512, 10))"
      ],
      "metadata": {
        "id": "DDbcReV10VSu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training ResNet, let us observe how the input shape changes across different modules in ResNet. As in all the previous architectures, the resolution decreases while the number of channels increases up until the point where a global average pooling layer aggregates all features."
      ],
      "metadata": {
        "id": "p9Ldfl0s0Xl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(size=(1, 1, 224, 224))\n",
        "for layer in net:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
      ],
      "metadata": {
        "id": "_BF5NeGp0aSV",
        "outputId": "74f94c74-d5bd-4e1d-ade9-16869d6b388f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
            "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
            "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
            "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
            "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
            "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
            "Flatten output shape:\t torch.Size([1, 512])\n",
            "Linear output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "vIPmHGMc0dKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train ResNet on the Fashion-MNIST dataset."
      ],
      "metadata": {
        "id": "frbMOejB0jkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr, num_epochs, batch_size = 0.05, 10, 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
        "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
      ],
      "metadata": {
        "id": "Ow9xtaZ40mEw",
        "outputId": "14cb8cb6-90cb-4a46-a50b-f68e3b35dbfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary**"
      ],
      "metadata": {
        "id": "pgsadfUq0xkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   ested function classes are desirable. Learning an additional layer in deep neural networks as an identity function (though this is an extreme case) should be made easy.\n",
        "\n",
        "*   The residual mapping can learn the identity function more easily, such as pushing parameters in the weight layer to zero.\n",
        "\n",
        "*   We can train an effective deep neural network by having residual blocks. Inputs can forward propagate faster through the residual connections across layers.\n",
        "*   ResNet had a major influence on the design of subsequent deep neural networks, both for convolutional and sequential nature.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ES3GnBWQ00sZ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "colab-github-demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}